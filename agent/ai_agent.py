import warnings
import os
import asyncio
import time
import traceback

# æŠ‘åˆ¶LangChainå¼ƒç”¨è­¦å‘Š
import warnings
import os
import asyncio
import concurrent.futures
import traceback
warnings.filterwarnings("ignore", category=DeprecationWarning)

from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, AIMessage, SystemMessage, BaseMessage
from dotenv import load_dotenv

# MCP å·¥å…·å¯¼å…¥
try:
    from mcp import ClientSession, StdioServerParameters
    from mcp.client.stdio import stdio_client
    from langchain_mcp_adapters.tools import load_mcp_tools
    from langgraph.prebuilt import create_react_agent
    MCP_AVAILABLE = True
except ImportError:
    print("MCPå·¥å…·ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨å¤‡ç”¨æ¨¡å¼")
    MCP_AVAILABLE = False

# å¯¼å…¥æç¤ºè¯
try:
    from .prompts import (
        GENERAL_SYSTEM_PROMPT, 
        TRAVEL_SYSTEM_PROMPT, 
        PDF_PROMPT,
        INFORMATION_COLLECTOR_PROMPT,
        ITINERARY_PLANNER_PROMPT
    )
except ImportError:
    from prompts import (
        GENERAL_SYSTEM_PROMPT, 
        TRAVEL_SYSTEM_PROMPT, 
        PDF_PROMPT,
        INFORMATION_COLLECTOR_PROMPT,
        ITINERARY_PLANNER_PROMPT
    )

load_dotenv()

# å…¨å±€å˜é‡
user_agents = {}
mcp_tools = []

# =============================================================================
# MCP å·¥å…·åŠ è½½
# =============================================================================
mcp_tools = []
user_agents = {}

async def load_mcp_tools_async():
    """å¼‚æ­¥åŠ è½½MCPå·¥å…·"""
    if not MCP_AVAILABLE:
        print("MCPå·¥å…·ä¸å¯ç”¨ï¼Œè¿”å›ç©ºå·¥å…·åˆ—è¡¨")
        return []
        
    try:
        # ç›´æ¥ä½¿ç”¨ç»å¯¹è·¯å¾„
        mcp_server_path = "mcp_server.py"
        
        if not os.path.exists(mcp_server_path):
            print(f"è­¦å‘Š: MCPæœåŠ¡å™¨æ–‡ä»¶ä¸å­˜åœ¨: {mcp_server_path}")
            return []
        
        # åˆ›å»ºæœåŠ¡å™¨å‚æ•°
        server_params = StdioServerParameters(
            command="python",
            args=[mcp_server_path],
            env={"SEARCHAPI_API_KEY": os.getenv("SEARCHAPI_API_KEY", "")}
        )
        
        # ä½¿ç”¨æ ‡å‡†MCPé€‚é…å™¨åŠ è½½å·¥å…·
        async with stdio_client(server_params) as (read, write):
            async with ClientSession(read, write) as session:
                # åˆå§‹åŒ–è¿æ¥
                await session.initialize()
                
                # è·å–å·¥å…·
                tools = await load_mcp_tools(session)
                print(f"å¼‚æ­¥åŠ è½½äº† {len(tools)} ä¸ªMCPå·¥å…·")
                return tools
                
    except Exception as e:
        print(f"å¼‚æ­¥åŠ è½½MCPå·¥å…·å¤±è´¥: {e}")
        return []

def load_mcp_tools_sync():
    """åŒæ­¥åŠ è½½MCPå·¥å…·çš„åŒ…è£…å™¨"""
    try:
        # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            tools = loop.run_until_complete(load_mcp_tools_async())
            return tools
        finally:
            loop.close()
    except Exception as e:
        print(f"åŒæ­¥åŠ è½½MCPå·¥å…·å¤±è´¥: {e}")
        return []

class MultiAgentTravelPlanner:
    """å¤šæ™ºèƒ½ä½“æ—…è¡Œè§„åˆ’ç³»ç»Ÿï¼ˆLangGraphç‰ˆï¼‰"""
    
    def __init__(self, user_email: str):
        self.user_email = user_email
        load_dotenv()
        
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.base_url = os.getenv("OPENAI_API_URL")
        
        if not self.api_key:
            raise ValueError("æœªé…ç½®OpenAI APIå¯†é’¥")
        
        self.llm = ChatOpenAI(
            api_key=self.api_key,
            model="gpt-4.1",
            base_url=self.base_url,
            streaming=True
        )
        
        global mcp_tools
        if not mcp_tools:
            print("åŠ è½½MCPå·¥å…·...")
            mcp_tools = load_mcp_tools_sync()
        
        self.tools = mcp_tools
        print(f"å¯ç”¨å·¥å…·æ•°é‡: {len(self.tools)}")
        
        # åˆ›å»ºLangGraphæ™ºèƒ½ä½“
        if self.tools:
            self.agent = create_react_agent(self.llm, self.tools)
            print("åˆ›å»ºäº†LangGraphæ™ºèƒ½ä½“")
        else:
            self.agent = None
            print("æœªåˆ›å»ºæ™ºèƒ½ä½“ï¼Œå·¥å…·åŠ è½½å¤±è´¥")
    
    def get_response_stream(self, message: str, system_prompt: str = TRAVEL_SYSTEM_PROMPT):
        """è·å–æµå¼å“åº”ï¼ˆä½¿ç”¨MCPå·¥å…·çš„LangGraphæ™ºèƒ½ä½“ï¼‰"""
        try:
            # å¦‚æœæœ‰å¯ç”¨çš„MCPå·¥å…·å’ŒLangGraphæ™ºèƒ½ä½“ï¼Œä½¿ç”¨å®ƒä»¬
            if self.agent and self.tools:
                print("MultiAgentTravelPlanner: ä½¿ç”¨MCPæ™ºèƒ½ä½“å¤„ç†è¯·æ±‚")
                
                def run_agent_sync():
                    """åŒæ­¥åŒ…è£…å™¨æ¥è¿è¡Œå¼‚æ­¥æ™ºèƒ½ä½“"""
                    async def run_agent():
                        # æ„å»ºæ¶ˆæ¯ï¼ŒåŒ…å«ç³»ç»Ÿæç¤ºå’Œç”¨æˆ·æ¶ˆæ¯
                        messages = [
                            SystemMessage(content=system_prompt),
                            HumanMessage(content=message)
                        ]
                        
                        # è°ƒç”¨LangGraphæ™ºèƒ½ä½“
                        response = await self.agent.ainvoke({"messages": messages})
                        
                        # æå–æœ€åä¸€æ¡AIæ¶ˆæ¯çš„å†…å®¹
                        if response and "messages" in response:
                            last_message = response["messages"][-1]
                            if hasattr(last_message, 'content'):
                                return last_message.content
                            elif isinstance(last_message, dict) and 'content' in last_message:
                                return last_message['content']
                        
                        return "æŠ±æ­‰ï¼Œæœªèƒ½è·å–åˆ°æœ‰æ•ˆå“åº”ã€‚"
                    
                    # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯æ¥è¿è¡Œå¼‚æ­¥ä»£ç 
                    new_loop = asyncio.new_event_loop()
                    try:
                        asyncio.set_event_loop(new_loop)
                        return new_loop.run_until_complete(run_agent())
                    finally:
                        new_loop.close()
                
                # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œå¼‚æ­¥ä»£ç 
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(run_agent_sync)
                    full_response = future.result(timeout=60)
            else:
                # å¦‚æœæ²¡æœ‰MCPå·¥å…·ï¼Œä½¿ç”¨æ™®é€šæ¨¡å‹
                print("MultiAgentTravelPlanner: æœªåŠ è½½MCPå·¥å…·ï¼Œä½¿ç”¨æ™®é€šæ¨¡å‹")
                non_streaming_llm = ChatOpenAI(
                    api_key=self.api_key,
                    model="gpt-4.1",
                    base_url=self.base_url,
                    streaming=True
                )
                
                messages = [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=message)
                ]
                
                # è·å–å®Œæ•´å“åº”
                response = non_streaming_llm.invoke(messages)
                full_response = response.content
            
            # æ¨¡æ‹Ÿæµå¼è¾“å‡º
            chunk_size = 50  # æ¯æ¬¡è¾“å‡º50ä¸ªå­—ç¬¦
            for i in range(0, len(full_response), chunk_size):
                chunk = full_response[i:i+chunk_size]
                yield chunk
                        
        except Exception as e:
            yield f"å¤„ç†è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {str(e)}"

class SimpleMemory:
    """ç®€å•çš„å†…å­˜å­˜å‚¨"""
    def __init__(self):
        self.messages = []
    
    def add_message(self, role, content):
        self.messages.append({"role": role, "content": content})
        # é™åˆ¶è®°å¿†é•¿åº¦
        if len(self.messages) > 60:  # ä¿ç•™æœ€è¿‘30è½®å¯¹è¯
            self.messages = self.messages[-60:]

def get_agent_response_stream(user_message, user_email, agent_type="general", conv_id=None):
    """è·å–æ™ºèƒ½ä½“å“åº”æµï¼ˆä½¿ç”¨é…å¤‡MCPå·¥å…·çš„LangGraphæ™ºèƒ½ä½“ï¼‰"""
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")
    base_url = os.getenv("OPENAI_API_URL")
    
    if not api_key:
        raise ValueError("OPENAI API key not configured")
    
    # é€‰æ‹©ç³»ç»Ÿæç¤º
    if agent_type == "travel":
        system_prompt = TRAVEL_SYSTEM_PROMPT
    elif agent_type == "pdf_generator":
        system_prompt = PDF_PROMPT
    else:
        system_prompt = GENERAL_SYSTEM_PROMPT
    
    # ç”Ÿæˆæ™ºèƒ½ä½“æ ‡è¯†
    agent_key = f"{user_email}_{agent_type}_{conv_id}" if conv_id else f"{user_email}_{agent_type}"
    
    try:
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨æ™ºèƒ½ä½“å®ä¾‹
        if agent_key not in user_agents:
            # åˆ›å»ºæ–°çš„æ™ºèƒ½ä½“å®ä¾‹
            planner = MultiAgentTravelPlanner(user_email)
            memory = SimpleMemory()
            user_agents[agent_key] = {
                'planner': planner,
                'memory': memory,
                'agent_type': agent_type,
                'conv_id': conv_id
            }
        
        agent_instance = user_agents[agent_key]
        planner = agent_instance['planner']
        memory = agent_instance['memory']
        
        # å¦‚æœæœ‰å¯ç”¨çš„MCPå·¥å…·å’ŒLangGraphæ™ºèƒ½ä½“ï¼Œä½¿ç”¨å®ƒä»¬
        if planner.agent and planner.tools:
            print("ä½¿ç”¨MCPæ™ºèƒ½ä½“å¤„ç†è¯·æ±‚")
            # å¼‚æ­¥è¿è¡ŒLangGraphæ™ºèƒ½ä½“
            try:
                def run_agent_sync():
                    """åŒæ­¥åŒ…è£…å™¨æ¥è¿è¡Œå¼‚æ­¥æ™ºèƒ½ä½“"""
                    async def run_agent():
                        # æ„å»ºæ¶ˆæ¯ï¼ŒåŒ…å«ç³»ç»Ÿæç¤ºå’Œç”¨æˆ·æ¶ˆæ¯
                        messages = [
                            SystemMessage(content=system_prompt),
                            HumanMessage(content=user_message)
                        ]
                        
                        print(f"è°ƒç”¨LangGraphæ™ºèƒ½ä½“ï¼Œæ¶ˆæ¯æ•°é‡: {len(messages)}")
                        # è°ƒç”¨LangGraphæ™ºèƒ½ä½“
                        response = await planner.agent.ainvoke({"messages": messages})
                        print(f"æ”¶åˆ°æ™ºèƒ½ä½“å“åº”: {type(response)}")
                        
                        # æå–æœ€åä¸€æ¡AIæ¶ˆæ¯çš„å†…å®¹
                        if response and "messages" in response:
                            last_message = response["messages"][-1]
                            if hasattr(last_message, 'content'):
                                return last_message.content
                            elif isinstance(last_message, dict) and 'content' in last_message:
                                return last_message['content']
                        
                        return "æŠ±æ­‰ï¼Œæœªèƒ½è·å–åˆ°æœ‰æ•ˆå“åº”ã€‚"
                    
                    # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯æ¥è¿è¡Œå¼‚æ­¥ä»£ç 
                    new_loop = asyncio.new_event_loop()
                    try:
                        asyncio.set_event_loop(new_loop)
                        return new_loop.run_until_complete(run_agent())
                    finally:
                        new_loop.close()
                        
                # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œå¼‚æ­¥ä»£ç 
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(run_agent_sync)
                    full_response = future.result(timeout=60)
                        
            except Exception as agent_error:
                print(f"LangGraphæ™ºèƒ½ä½“è°ƒç”¨å¤±è´¥: {agent_error}")
                # å›é€€åˆ°æ™®é€šæ¨¡å‹
                model = ChatOpenAI(
                    api_key=api_key,
                    model="gpt-4.1",
                    base_url=base_url,
                    streaming=False
                )
                
                messages = [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=user_message)
                ]
                
                response = model.invoke(messages)
                full_response = response.content
        
        
        try:
            if hasattr(memory, 'add_message'):
                memory.add_message("user", user_message)
                memory.add_message("assistant", full_response)
            else:
                print("å†…å­˜å¯¹è±¡æ²¡æœ‰add_messageæ–¹æ³•")
        except Exception as e:
            print(f"ä¿å­˜è®°å¿†å¤±è´¥: {e}")
        
        # å°†å®Œæ•´å“åº”åˆ†æˆå°å—è¿›è¡Œæµå¼è¾“å‡º
        chunk_size = 50  # æ¯æ¬¡è¾“å‡º50ä¸ªå­—ç¬¦
        for i in range(0, len(full_response), chunk_size):
            chunk = full_response[i:i+chunk_size]
            yield chunk
                    
    except Exception as e:
        # æ¸…ç†æ— æ•ˆçš„æ™ºèƒ½ä½“å®ä¾‹
        if agent_key in user_agents:
            del user_agents[agent_key]
        print(f"æ™ºèƒ½ä½“å“åº”å‡ºé”™: {e}")
        # æä¾›å¤‡ç”¨å“åº”
        yield f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°äº†é—®é¢˜: {str(e)}"

def stream_model_response_sync(model, system_prompt, user_message):
    """åŒæ­¥æµå¼è°ƒç”¨æ¨¡å‹å“åº”"""
    try:
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_message)
        ]
        
        for chunk in model.stream(messages):
            if hasattr(chunk, 'content') and chunk.content:
                yield chunk.content
                
    except Exception as e:
        yield f"æ¨¡å‹è°ƒç”¨å¤±è´¥: {str(e)}"

def is_travel_planning_request(message: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºæ—…è¡Œè§„åˆ’è¯·æ±‚"""
    travel_keywords = [
        'æ—…è¡Œ', 'æ—…æ¸¸', 'å‡ºè¡Œ', 'è¡Œç¨‹', 'è§„åˆ’', 'è®¡åˆ’',
        'æœºç¥¨', 'é…’åº—', 'ä½å®¿', 'æ™¯ç‚¹', 'è·¯çº¿',
        'travel', 'trip', 'vacation', 'itinerary', 'plan',
        'flight', 'hotel', 'attraction', 'route'
    ]
    
    message_lower = message.lower()
    return any(keyword in message_lower for keyword in travel_keywords)

def handle_travel_planning_stream(travel_request: str, user_email: str, messages: list[BaseMessage], conv_id=None):
    """å¤„ç†æ—…è¡Œç›¸å…³è¯·æ±‚çš„æµå¼å“åº”"""
    # æ„å»ºå®Œæ•´å“åº”
    full_response = ""
    
    try:
        # ç›´æ¥ä½¿ç”¨ç³»ç»Ÿæç¤ºå’Œæ¶ˆæ¯æ„å»ºå®Œæ•´å“åº”
        load_dotenv()
        api_key = os.getenv("OPENAI_API_KEY")
        base_url = os.getenv("OPENAI_API_URL")
        
        model = ChatOpenAI(
            api_key=api_key,
            model="gpt-4.1",
            base_url=base_url,
            streaming=False  # å…³é—­æµå¼å¤„ç†ï¼Œè·å–å®Œæ•´å“åº”
        )
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºå®Œæ•´çš„æ—…è¡Œè§„åˆ’è¯·æ±‚
        if is_travel_planning_request(travel_request):
            full_response = "\n\n## ğŸ” æ­£åœ¨åˆ¶å®šæ—…è¡Œè®¡åˆ’...\n\n"
            
            # è·å–å®Œæ•´å“åº”
            messages = [
                SystemMessage(content=TRAVEL_SYSTEM_PROMPT),
                HumanMessage(content=travel_request)
            ]
            response = model.invoke(messages)
            full_response += response.content
        else:
            # ä¸€èˆ¬çš„æ—…è¡Œé—®é¢˜
            messages = [
                SystemMessage(content=TRAVEL_SYSTEM_PROMPT),
                HumanMessage(content=travel_request)
            ]
            response = model.invoke(messages)
            full_response = response.content
        
        # æµå¼è¿”å›å®Œæ•´å“åº”
        chunk_size = 50  # æ¯æ¬¡è¾“å‡º50ä¸ªå­—ç¬¦
        for i in range(0, len(full_response), chunk_size):
            chunk = full_response[i:i+chunk_size]
            yield chunk
            
    except Exception as e:
        yield f"\n\nâŒ æ—…è¡Œè§„åˆ’è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}\n\n"

def get_agent_response(user_message, user_email, agent_type="general", conv_id=None):
    """è·å–å®Œæ•´çš„AIå“åº”ï¼ˆéæµå¼ï¼‰"""
    response_text = ""
    for chunk in get_agent_response_stream(user_message, user_email, agent_type, conv_id):
        response_text += chunk
    return response_text

async def get_mcp_response_async(user_message, user_email, agent_type="general"):
    """å¼‚æ­¥è·å–MCPå“åº”ï¼ˆå®Œå…¨æ¨¡ä»¿test_mcp.pyçš„æˆåŠŸæ¨¡å¼ï¼‰"""
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")
    base_url = os.getenv("OPENAI_API_URL")
    
    if not api_key:
        raise ValueError("OPENAI API key not configured")
    
    try:
        print(f"[MCP] å¼€å§‹MCPè°ƒç”¨ï¼Œç”¨æˆ·: {user_email}, ç±»å‹: {agent_type}")
        
        # MCPæœåŠ¡å™¨è·¯å¾„ï¼ˆä¸test_mcp.pyå®Œå…¨ç›¸åŒï¼‰
        mcp_server_path = "mcp_server.py"
        
        if not os.path.exists(mcp_server_path):
            print(f"[MCP] é”™è¯¯: MCPæœåŠ¡å™¨æ–‡ä»¶ä¸å­˜åœ¨: {mcp_server_path}")
            return None
        
        print(f"[MCP] MCPæœåŠ¡å™¨è·¯å¾„: {mcp_server_path}")
        
        # åˆ›å»ºæœåŠ¡å™¨å‚æ•°ï¼ˆä¸test_mcp.pyå®Œå…¨ç›¸åŒï¼‰
        server_params = StdioServerParameters(
            command="python",
            args=[mcp_server_path],
            env={"SEARCHAPI_API_KEY": os.getenv("SEARCHAPI_API_KEY", "")}
        )
        
        print("[MCP] æ­£åœ¨è¿æ¥MCPæœåŠ¡å™¨...")
        
        # ä½¿ç”¨æ ‡å‡†MCPé€‚é…å™¨åŠ è½½å·¥å…·ï¼ˆä¸test_mcp.pyå®Œå…¨ç›¸åŒï¼‰
        async with stdio_client(server_params) as (read, write):
            async with ClientSession(read, write) as session:
                print("[MCP] MCPæœåŠ¡å™¨è¿æ¥æˆåŠŸï¼Œæ­£åœ¨åˆå§‹åŒ–...")
                
                # åˆå§‹åŒ–è¿æ¥
                await session.initialize()
                print("[MCP] MCPä¼šè¯åˆå§‹åŒ–å®Œæˆ")
                
                # è·å–å·¥å…·
                tools = await load_mcp_tools(session)
                print(f"[MCP] æˆåŠŸåŠ è½½ {len(tools)} ä¸ªMCPå·¥å…·:")
                
                for i, tool in enumerate(tools, 1):
                    print(f"[MCP]   {i}. {tool.name}: {tool.description}")
                
                if not tools:
                    print("[MCP] è­¦å‘Š: æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•å·¥å…·")
                    return None
                
                # åˆ›å»ºLLMï¼ˆä¸test_mcp.pyå®Œå…¨ç›¸åŒï¼‰
                llm = ChatOpenAI(
                    temperature=0.1,
                    api_key=api_key,
                    model="gpt-4.1",
                    base_url=base_url,
                    streaming=False
                )
                
                print("[MCP] æ­£åœ¨åˆ›å»ºLangGraphæ™ºèƒ½ä½“...")
                
                # åˆ›å»ºæ™ºèƒ½ä½“ï¼ˆä¸test_mcp.pyå®Œå…¨ç›¸åŒï¼‰
                agent = create_react_agent(llm, tools)
                print("[MCP] LangGraphæ™ºèƒ½ä½“åˆ›å»ºæˆåŠŸ")
                
                # æµ‹è¯•æŸ¥è¯¢ï¼ˆä¸test_mcp.pyå®Œå…¨ç›¸åŒçš„æ¶ˆæ¯æ ¼å¼ï¼‰
                print(f"\n[MCP] å¼€å§‹å¤„ç†æŸ¥è¯¢: {user_message[:100]}...")
                
                response = await agent.ainvoke({"messages": [{"role": "user", "content": user_message}]})
                
                print("\n[MCP] æ™ºèƒ½ä½“å“åº”:")
                if response and "messages" in response:
                    last_message = response["messages"][-1]
                    if hasattr(last_message, 'content'):
                        content = last_message.content
                        print(f"[MCP] å“åº”å†…å®¹é•¿åº¦: {len(content)}")
                        print(f"[MCP] å“åº”å†…å®¹å‰200å­—ç¬¦: {content[:200]}...")
                        return content
                    else:
                        print(f"[MCP] å“åº”ç±»å‹: {type(last_message)}")
                        print(f"[MCP] å“åº”å†…å®¹: {last_message}")
                        return str(last_message)
                else:
                    print(f"[MCP] æ„å¤–çš„å“åº”æ ¼å¼: {response}")
                    return None
                
                print("\n[MCP] MCPå·¥å…·è°ƒç”¨å®Œæˆï¼")
                
    except Exception as e:
        print(f"[MCP] è°ƒç”¨è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None

def get_mcp_response_sync(user_message, user_email, agent_type="general"):
    """åŒæ­¥è°ƒç”¨MCPå“åº”"""
    try:
        return asyncio.run(get_mcp_response_async(user_message, user_email, agent_type))
    except Exception as e:
        print(f"[MCP] åŒæ­¥è°ƒç”¨å¤±è´¥: {e}")
        return None

async def multi_agent_travel_planning_async(user_request, user_email):
    """å¤šæ™ºèƒ½ä½“æ—…è¡Œè§„åˆ’æµæ°´çº¿ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")
    base_url = os.getenv("OPENAI_API_URL")
    
    if not api_key:
        raise ValueError("OPENAI API key not configured")
    
    try:
        print(f"[å¤šæ™ºèƒ½ä½“] å¼€å§‹å¤šæ™ºèƒ½ä½“æ—…è¡Œè§„åˆ’ï¼Œç”¨æˆ·: {user_email}")
        
        # MCPæœåŠ¡å™¨è·¯å¾„
        mcp_server_path = "mcp_server.py"
        
        if not os.path.exists(mcp_server_path):
            print(f"[å¤šæ™ºèƒ½ä½“] é”™è¯¯: MCPæœåŠ¡å™¨æ–‡ä»¶ä¸å­˜åœ¨: {mcp_server_path}")
            return None
        
        # åˆ›å»ºæœåŠ¡å™¨å‚æ•°
        server_params = StdioServerParameters(
            command="python",
            args=[mcp_server_path],
            env={"SEARCHAPI_API_KEY": os.getenv("SEARCHAPI_API_KEY", "")}
        )
        
        print("[å¤šæ™ºèƒ½ä½“] æ­£åœ¨è¿æ¥MCPæœåŠ¡å™¨...")
        
        # ä½¿ç”¨MCPå·¥å…·
        async with stdio_client(server_params) as (read, write):
            async with ClientSession(read, write) as session:
                print("[å¤šæ™ºèƒ½ä½“] MCPæœåŠ¡å™¨è¿æ¥æˆåŠŸï¼Œæ­£åœ¨åˆå§‹åŒ–...")
                
                # åˆå§‹åŒ–è¿æ¥
                await session.initialize()
                print("[å¤šæ™ºèƒ½ä½“] MCPä¼šè¯åˆå§‹åŒ–å®Œæˆ")
                
                # è·å–å·¥å…·
                tools = await load_mcp_tools(session)
                print(f"[å¤šæ™ºèƒ½ä½“] æˆåŠŸåŠ è½½ {len(tools)} ä¸ªMCPå·¥å…·")
                
                if not tools:
                    print("[å¤šæ™ºèƒ½ä½“] è­¦å‘Š: æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•å·¥å…·")
                    return None
                
                # åˆ›å»ºLLM
                llm = ChatOpenAI(
                    api_key=api_key,
                    model="gpt-4.1",
                    base_url=base_url,
                    streaming=False
                )
                
                # é˜¶æ®µ1: ä¿¡æ¯æ”¶é›†æ™ºèƒ½ä½“
                print("\n[å¤šæ™ºèƒ½ä½“] === é˜¶æ®µ1: ä¿¡æ¯æ”¶é›†æ™ºèƒ½ä½“ ===")
                print("[å¤šæ™ºèƒ½ä½“] æ­£åœ¨åˆ›å»ºä¿¡æ¯æ”¶é›†æ™ºèƒ½ä½“...")
                
                collector_agent = create_react_agent(llm, tools)
                print("[å¤šæ™ºèƒ½ä½“] ä¿¡æ¯æ”¶é›†æ™ºèƒ½ä½“åˆ›å»ºæˆåŠŸ")
                
                # æ„å»ºä¿¡æ¯æ”¶é›†è¯·æ±‚
                collector_request = f"""
{INFORMATION_COLLECTOR_PROMPT}

ç”¨æˆ·æ—…è¡Œéœ€æ±‚ï¼š
{user_request}

è¯·ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„æœç´¢å·¥å…·æ”¶é›†ç›¸å…³çš„æ—…è¡Œä¿¡æ¯ã€‚
"""
                
                print(f"[å¤šæ™ºèƒ½ä½“] å¼€å§‹ä¿¡æ¯æ”¶é›†ï¼Œè¯·æ±‚é•¿åº¦: {len(collector_request)}")
                
                # æ‰§è¡Œä¿¡æ¯æ”¶é›†
                collector_response = await collector_agent.ainvoke({
                    "messages": [{"role": "user", "content": collector_request}]
                })
                
                # æå–ä¿¡æ¯æ”¶é›†ç»“æœ
                collected_info = ""
                if collector_response and "messages" in collector_response:
                    last_message = collector_response["messages"][-1]
                    if hasattr(last_message, 'content'):
                        collected_info = last_message.content
                    else:
                        collected_info = str(last_message)
                
                print(f"[å¤šæ™ºèƒ½ä½“] ä¿¡æ¯æ”¶é›†å®Œæˆï¼Œæ”¶é›†åˆ°çš„ä¿¡æ¯é•¿åº¦: {len(collected_info)}")
                print(f"[å¤šæ™ºèƒ½ä½“] ä¿¡æ¯æ”¶é›†å‰300å­—ç¬¦: {collected_info[:300]}...")
                
                # é˜¶æ®µ2: è¡Œç¨‹è§„åˆ’æ™ºèƒ½ä½“
                print("\n[å¤šæ™ºèƒ½ä½“] === é˜¶æ®µ2: è¡Œç¨‹è§„åˆ’æ™ºèƒ½ä½“ ===")
                print("[å¤šæ™ºèƒ½ä½“] æ­£åœ¨åˆ›å»ºè¡Œç¨‹è§„åˆ’æ™ºèƒ½ä½“...")
                
                planner_agent = create_react_agent(llm, tools)
                print("[å¤šæ™ºèƒ½ä½“] è¡Œç¨‹è§„åˆ’æ™ºèƒ½ä½“åˆ›å»ºæˆåŠŸ")
                
                # æ„å»ºè¡Œç¨‹è§„åˆ’è¯·æ±‚
                planner_request = f"""
{ITINERARY_PLANNER_PROMPT}

## ç”¨æˆ·åŸå§‹éœ€æ±‚ï¼š
{user_request}

## ä¿¡æ¯æ”¶é›†æ™ºèƒ½ä½“æä¾›çš„è¯¦ç»†ä¿¡æ¯ï¼š
{collected_info}

è¯·åŸºäºä»¥ä¸Šæ”¶é›†åˆ°çš„è¯¦ç»†ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·åˆ¶å®šå®Œæ•´çš„ä¸ªæ€§åŒ–æ—…è¡Œæ–¹æ¡ˆã€‚
è¯·ç¡®ä¿å……åˆ†åˆ©ç”¨æ”¶é›†åˆ°çš„å…·ä½“ä¿¡æ¯ï¼ˆä»·æ ¼ã€åœ°å€ã€æ—¶é—´ç­‰ï¼‰ï¼Œåˆ¶å®šå¯æ‰§è¡Œçš„æ—…è¡Œè®¡åˆ’ã€‚
"""
                
                print(f"[å¤šæ™ºèƒ½ä½“] å¼€å§‹è¡Œç¨‹è§„åˆ’ï¼Œè¯·æ±‚é•¿åº¦: {len(planner_request)}")
                
                # æ‰§è¡Œè¡Œç¨‹è§„åˆ’
                planner_response = await planner_agent.ainvoke({
                    "messages": [{"role": "user", "content": planner_request}]
                })
                
                # æå–è¡Œç¨‹è§„åˆ’ç»“æœ
                travel_plan = ""
                if planner_response and "messages" in planner_response:
                    last_message = planner_response["messages"][-1]
                    if hasattr(last_message, 'content'):
                        travel_plan = last_message.content
                    else:
                        travel_plan = str(last_message)
                
                print(f"[å¤šæ™ºèƒ½ä½“] è¡Œç¨‹è§„åˆ’å®Œæˆï¼Œè§„åˆ’æ–¹æ¡ˆé•¿åº¦: {len(travel_plan)}")
                print(f"[å¤šæ™ºèƒ½ä½“] è§„åˆ’æ–¹æ¡ˆå‰300å­—ç¬¦: {travel_plan[:300]}...")
                
                # åˆå¹¶æœ€ç»ˆç»“æœ
                final_result = f"""# ğŸ¯ ä¸“ä¸šæ—…è¡Œè§„åˆ’æ–¹æ¡ˆ

## ğŸ“‹ è§„åˆ’æµç¨‹è¯´æ˜
æœ¬æ–¹æ¡ˆé€šè¿‡ä¸¤ä¸ªä¸“ä¸šæ™ºèƒ½ä½“åä½œå®Œæˆï¼š
1. **ä¿¡æ¯æ”¶é›†æ™ºèƒ½ä½“**ï¼šä½¿ç”¨å®æ—¶æœç´¢å·¥å…·æ”¶é›†æœ€æ–°çš„æ—…è¡Œä¿¡æ¯
2. **è¡Œç¨‹è§„åˆ’æ™ºèƒ½ä½“**ï¼šåŸºäºæ”¶é›†çš„ä¿¡æ¯åˆ¶å®šä¸ªæ€§åŒ–æ—…è¡Œæ–¹æ¡ˆ

---

{travel_plan}

---

## ğŸ“Š ä¿¡æ¯æ”¶é›†è¯¦æƒ…
<details>
<summary>ç‚¹å‡»æŸ¥çœ‹è¯¦ç»†çš„ä¿¡æ¯æ”¶é›†è¿‡ç¨‹</summary>

{collected_info}

</details>

---

*æœ¬è§„åˆ’æ–¹æ¡ˆç”±AIå¤šæ™ºèƒ½ä½“ç³»ç»Ÿç”Ÿæˆï¼ŒåŸºäºå®æ—¶æœç´¢çš„æœ€æ–°ä¿¡æ¯åˆ¶å®šã€‚*
"""
                
                print(f"[å¤šæ™ºèƒ½ä½“] å¤šæ™ºèƒ½ä½“è§„åˆ’å®Œæˆï¼Œæœ€ç»ˆç»“æœé•¿åº¦: {len(final_result)}")
                return final_result
                
    except Exception as e:
        print(f"[å¤šæ™ºèƒ½ä½“] å¤šæ™ºèƒ½ä½“è§„åˆ’è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None

def multi_agent_travel_planning_sync(user_request, user_email):
    """å¤šæ™ºèƒ½ä½“æ—…è¡Œè§„åˆ’æµæ°´çº¿ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
    try:
        return asyncio.run(multi_agent_travel_planning_async(user_request, user_email))
    except Exception as e:
        print(f"[å¤šæ™ºèƒ½ä½“] åŒæ­¥è°ƒç”¨å¤±è´¥: {e}")
        return None
