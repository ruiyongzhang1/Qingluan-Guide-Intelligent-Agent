import warnings
import os
import asyncio
import time
import traceback
import concurrent.futures
from typing import Dict, Any, List, Optional, Generator

# æŠ‘åˆ¶LangChainå¼ƒç”¨è­¦å‘Š
warnings.filterwarnings("ignore", category=DeprecationWarning)

from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, AIMessage, SystemMessage, BaseMessage
from dotenv import load_dotenv

# MCP å·¥å…·å¯¼å…¥
try:
    from mcp import ClientSession, StdioServerParameters
    from mcp.client.stdio import stdio_client
    from langchain_mcp_adapters.tools import load_mcp_tools
    from langgraph.prebuilt import create_react_agent
    MCP_AVAILABLE = True
except ImportError:
    print("MCPå·¥å…·ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨å¤‡ç”¨æ¨¡å¼")
    MCP_AVAILABLE = False

# PDFç”Ÿæˆç±»å’Œæç¤ºè¯å¯¼å…¥
from agent.pdf_generator import PDFGeneratorTool
try:
    from .prompts import (
        GENERAL_SYSTEM_PROMPT, TRAVEL_SYSTEM_PROMPT, PDF_PROMPT,
        INFORMATION_COLLECTOR_PROMPT, ITINERARY_PLANNER_PROMPT
    )
    from .redis_memory import get_redis_memory_manager, SimpleMemory as RedisSimpleMemory
except ImportError:
    from agent.prompts import (
        GENERAL_SYSTEM_PROMPT, TRAVEL_SYSTEM_PROMPT, PDF_PROMPT,
        INFORMATION_COLLECTOR_PROMPT, ITINERARY_PLANNER_PROMPT
    )
    from agent.redis_memory import get_redis_memory_manager, SimpleMemory as RedisSimpleMemory

# =============================================================================
# 1. Component and Utility Classes (The Foundation)
# è¿™äº›æ˜¯æ„æˆç³»ç»Ÿçš„åŸºç¡€æ¨¡å—ï¼Œæ¯ä¸ªç±»èŒè´£å•ä¸€ã€‚
# =============================================================================

class ConfigManager:
    """ç»Ÿä¸€é…ç½®ç®¡ç†"""
    def __init__(self):
        load_dotenv()
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.base_url = os.getenv("OPENAI_API_URL")
        self.searchapi_key = os.getenv("SEARCHAPI_API_KEY", "")
        self.mcp_server_path = "agent/mcp_server.py"
        
        if not self.api_key:
            raise ValueError("æœªé…ç½®OpenAI APIå¯†é’¥")
    
    def get_server_params(self) -> StdioServerParameters:
        """è·å–MCPæœåŠ¡å™¨å‚æ•°"""
        return StdioServerParameters(
            command="python",
            args=[self.mcp_server_path],
            env={"SEARCHAPI_API_KEY": self.searchapi_key}
        )

class LLMFactory:
    """LLMå®ä¾‹å·¥å‚"""
    def __init__(self, config: ConfigManager):
        self.config = config
    
    def create_llm(self, model: str = "gpt-4.1-nano", temperature: float = 0.1, 
                   streaming: bool = False) -> ChatOpenAI:
        """åˆ›å»ºLLMå®ä¾‹"""
        return ChatOpenAI(
            api_key=self.config.api_key,
            model=model,
            base_url=self.config.base_url,
            temperature=temperature,
            streaming=streaming
        )

class MCPManager:
    """MCPè¿æ¥å’Œå·¥å…·ç®¡ç†"""
    def __init__(self, config: ConfigManager):
        self.config = config
    
    async def load_tools_async(self) -> List[Any]:
        """å¼‚æ­¥åŠ è½½MCPå·¥å…·"""
        if not MCP_AVAILABLE or not os.path.exists(self.config.mcp_server_path):
            if not MCP_AVAILABLE: print("MCPä¾èµ–åº“ä¸å¯ç”¨ï¼Œè¿”å›ç©ºå·¥å…·åˆ—è¡¨")
            else: print(f"è­¦å‘Š: MCPæœåŠ¡å™¨æ–‡ä»¶ä¸å­˜åœ¨: {self.config.mcp_server_path}")
            return []
            
        try:
            server_params = self.config.get_server_params()
            async with stdio_client(server_params) as (read, write):
                async with ClientSession(read, write) as session:
                    await session.initialize()
                    tools = await load_mcp_tools(session)
                    print(f"å¼‚æ­¥åŠ è½½äº† {len(tools)} ä¸ªMCPå·¥å…·")
                    return tools
        except Exception as e:
            print(f"å¼‚æ­¥åŠ è½½MCPå·¥å…·å¤±è´¥: {e}")
            return []
    
    def load_tools_sync(self) -> List[Any]:
        """åŒæ­¥åŠ è½½MCPå·¥å…·çš„åŒ…è£…å™¨"""
        try:
            # Note: Using asyncio.run() is simpler and safer than managing loops directly
            return asyncio.run(self.load_tools_async())
        except Exception as e:
            print(f"åŒæ­¥åŠ è½½MCPå·¥å…·å¤±è´¥: {e}")
            return []

class AsyncSyncWrapper:
    """å¼‚æ­¥åŒæ­¥è½¬æ¢å·¥å…·"""
    @staticmethod
    def run_async_in_thread(async_func_or_coro, timeout: int = 60):
        """åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œå¼‚æ­¥å‡½æ•°æˆ–åç¨‹"""
        def sync_wrapper():
            if callable(async_func_or_coro):
                # å¦‚æœæ˜¯å‡½æ•°ï¼Œè°ƒç”¨å®ƒæ¥è·å¾—åç¨‹
                coro = async_func_or_coro()
            else:
                # å¦‚æœå·²ç»æ˜¯åç¨‹ï¼Œç›´æ¥ä½¿ç”¨
                coro = async_func_or_coro
            return asyncio.run(coro)
        
        with concurrent.futures.ThreadPoolExecutor() as executor:
            future = executor.submit(sync_wrapper)
            return future.result(timeout=timeout)

class StreamingUtils:
    """æµå¼è¾“å‡ºå·¥å…·"""
    @staticmethod
    def stream_text(text: str, chunk_size: int = 50) -> Generator[str, None, None]:
        """å°†æ–‡æœ¬åˆ†å—è¿›è¡Œæµå¼è¾“å‡º"""
        for i in range(0, len(text), chunk_size):
            yield text[i:i+chunk_size]

class ResponseExtractor:
    """å“åº”å†…å®¹æå–å·¥å…·"""
    @staticmethod
    def extract_agent_response(response: Dict[str, Any]) -> str:
        """ä»LangGraphæ™ºèƒ½ä½“å“åº”ä¸­æå–å†…å®¹"""
        if response and "messages" in response:
            last_message = response["messages"][-1]
            if hasattr(last_message, 'content'):
                return last_message.content
            elif isinstance(last_message, dict) and 'content' in last_message:
                return last_message['content']
        return "æŠ±æ­‰ï¼Œæœªèƒ½è·å–åˆ°æœ‰æ•ˆå“åº”ã€‚"

# =============================================================================
# 2. Refactored Agent Classes (The Workers)
# æ¯ä¸ªæ™ºèƒ½ä½“ç±»ç°åœ¨é€šè¿‡æ„é€ å‡½æ•°æ¥æ”¶å…¶ä¾èµ–é¡¹ï¼ˆå¦‚LLMå®ä¾‹å’Œå·¥å…·ï¼‰ï¼Œè€Œä¸æ˜¯è‡ªå·±åˆ›å»ºã€‚
# This is called Dependency Injection.
# =============================================================================

class InformationCollectorAgent:
    """ä¿¡æ¯æ”¶é›†æ™ºèƒ½ä½“ï¼ˆç»‘å®šMCPå·¥å…·ï¼‰"""
    def __init__(self, llm: ChatOpenAI, tools: List[Any]):
        self.llm = llm
        self.tools = tools
        self.agent = create_react_agent(self.llm, self.tools) if self.tools else None
        print(f"ä¿¡æ¯æ”¶é›†æ™ºèƒ½ä½“å·²åˆ›å»ºï¼Œå¯ç”¨å·¥å…·æ•°é‡: {len(self.tools)}")
    
    async def collect_information_async(self, user_request: str) -> str:
        if not self.agent:
            return f"ä¿¡æ¯æ”¶é›†æ™ºèƒ½ä½“ä¸å¯ç”¨ï¼ˆå·¥å…·åŠ è½½å¤±è´¥ï¼‰ï¼Œæ— æ³•å¤„ç†è¯·æ±‚: {user_request}"
        
        collector_request = f"{INFORMATION_COLLECTOR_PROMPT}\nç”¨æˆ·éœ€æ±‚:{user_request}"
        response = await self.agent.ainvoke({"messages": [{"role": "user", "content": collector_request}]})
        return ResponseExtractor.extract_agent_response(response)
    
    def get_response_stream(self, message: str):
        """è·å–å“åº”æµ"""
        try:
            full_response = AsyncSyncWrapper.run_async_in_thread(
                lambda: self.collect_information_async(message)
            )
            yield from StreamingUtils.stream_text(full_response)
        except Exception as e:
            yield f"å¤„ç†è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {str(e)}"

class PlannerAgent:
    """è¡Œç¨‹è§„åˆ’æ™ºèƒ½ä½“"""
    def __init__(self, llm_streaming: ChatOpenAI, llm_normal: ChatOpenAI):
        self.llm_streaming = llm_streaming
        self.llm_normal = llm_normal
        print("è¡Œç¨‹è§„åˆ’æ™ºèƒ½ä½“å·²åˆ›å»º")
        
    def get_response_stream(self, message: str, collected_info: str = "", conversation_history: list = None):
        """è·å–çœŸæµå¼å“åº”ï¼ˆæ”¯æŒå¯¹è¯è®°å¿†ï¼‰"""
        # æ„å»ºè§„åˆ’è¯·æ±‚å†…å®¹
        if collected_info:
            planning_content = f"ç”¨æˆ·åŸå§‹éœ€æ±‚ï¼š\n{message}\n\nä¿¡æ¯æ”¶é›†æ™ºèƒ½ä½“æä¾›çš„è¯¦ç»†ä¿¡æ¯ï¼š\n{collected_info}"
        else:
            planning_content = f"ç”¨æˆ·éœ€æ±‚ï¼š\n{message}"
        
        # æ„å»ºåŒ…å«å†å²è®°å¿†çš„æ¶ˆæ¯åˆ—è¡¨
        messages = [SystemMessage(content=ITINERARY_PLANNER_PROMPT)]
        
        # æ·»åŠ å†å²å¯¹è¯è®°å¿†ï¼ˆå…³äºæ—…è¡Œè§„åˆ’çš„ä¸Šä¸‹æ–‡ï¼‰
        if conversation_history:
            for msg in conversation_history[-8:]:  # æ—…è¡Œè§„åˆ’å¯èƒ½éœ€è¦æ›´å¤šä¸Šä¸‹æ–‡ï¼Œä½†æ§åˆ¶æ•°é‡
                role = msg.get('role', '')
                content = msg.get('content', '')
                if role == 'user':
                    messages.append(HumanMessage(content=content))
                elif role == 'assistant':
                    messages.append(AIMessage(content=content))
        
        # æ·»åŠ å½“å‰è§„åˆ’è¯·æ±‚
        messages.append(HumanMessage(content=planning_content))
        
        # çœŸæµå¼è°ƒç”¨LLM
        for chunk in self.llm_streaming.stream(messages):
            if hasattr(chunk, 'content') and chunk.content:
                yield chunk.content

class PdfAgent:
    """PDFç”Ÿæˆæ™ºèƒ½ä½“"""
    def __init__(self, llm: ChatOpenAI):
        self.llm = llm
        self.pdf_generator = PDFGeneratorTool()
        print("PDFç”Ÿæˆæ™ºèƒ½ä½“å·²åˆ›å»º")

    def generate_pdf(self, user_request: str, conversation_history: list = None) -> str:
        """ç”ŸæˆPDFæ—…æ¸¸æ”»ç•¥"""
        if not conversation_history:
            return "æš‚æ— å¯¹è¯å†å²è®°å½•ï¼Œæ— æ³•ç”ŸæˆPDFæŠ¥å‘Šã€‚"
        
        conversation_text = self._format_conversation_history(conversation_history)
        summary = self._generate_conversation_summary(conversation_text, user_request)
        detailed_guide = self._generate_travel_guide(conversation_text, user_request)
        full_content = f"# æ—…è¡Œå¯¹è¯è®°å½•\n\n{conversation_text}\n\n---\n\n# è¯¦ç»†æ—…æ¸¸æ”»ç•¥\n\n{detailed_guide}"
        pdf_result = self.pdf_generator.generate_travel_pdf(conversation_data=full_content, summary=summary, user_info="user") # user_info can be enhanced
        return f"ğŸ“„{pdf_result}"
    
    def _format_conversation_history(self, conversation_history: list) -> str:
        return "\n\n".join([f"**{msg.get('role', 'æœªçŸ¥')}**: {msg.get('content', '')}" for msg in conversation_history])

    def _generate_conversation_summary(self, conversation_text: str, user_request: str) -> str:
        prompt = f"è¯·å¯¹ä»¥ä¸‹æ—…è¡Œå¯¹è¯è¿›è¡Œæ€»ç»“ï¼Œæå–å…³é”®ä¿¡æ¯ï¼š\n{conversation_text}\nå½“å‰è¯·æ±‚ï¼š{user_request}\næ€»ç»“åº”ç®€æ´æ˜äº†ï¼Œä¸è¶…è¿‡200å­—ã€‚"
        response = self.llm.invoke([SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ—…è¡Œé¡¾é—®ï¼Œæ“…é•¿æ€»ç»“å’Œæç‚¼ä¿¡æ¯ã€‚"), HumanMessage(content=prompt)])
        return response.content
        
    def _generate_travel_guide(self, conversation_text: str, user_request: str) -> str:
        prompt = f"åŸºäºä»¥ä¸‹å¯¹è¯å†…å®¹ï¼Œç”Ÿæˆä¸€ä»½è¯¦ç»†çš„æ—…æ¸¸æ”»ç•¥ï¼š\n{conversation_text}\nå½“å‰éœ€æ±‚:{user_request}\nè¯·ç”Ÿæˆä¸€ä»½å®Œæ•´çš„æ—…æ¸¸æ”»ç•¥,ä½¿ç”¨èƒ½è®©pdfkitæ¸²æŸ“çš„markdownæ ¼å¼ã€‚"
        response = self.llm.invoke([SystemMessage(content=PDF_PROMPT), HumanMessage(content=prompt)])
        return response.content
    
    def get_response_stream(self, message: str, conversation_history: list):
        """è·å–å“åº”æµ"""
        full_response = self.generate_pdf(message, conversation_history)
        yield from StreamingUtils.stream_text(full_response)

class NormalAgent:
    """æ™®é€šå¯¹è¯æ™ºèƒ½ä½“"""
    def __init__(self, llm_streaming: ChatOpenAI):
        self.llm_streaming = llm_streaming
        print("æ™®é€šå¯¹è¯æ™ºèƒ½ä½“å·²åˆ›å»º")

    def get_response_stream(self, message: str, conversation_history: list = None):
        """è·å–çœŸæµå¼å“åº”ï¼ˆæ”¯æŒå¯¹è¯è®°å¿†ï¼‰"""
        # æ„å»ºåŒ…å«å†å²è®°å¿†çš„æ¶ˆæ¯åˆ—è¡¨
        messages = [SystemMessage(content=GENERAL_SYSTEM_PROMPT)]
        
        # æ·»åŠ å†å²å¯¹è¯è®°å¿†
        if conversation_history:
            for msg in conversation_history[-10:]:  # åªå–æœ€è¿‘10æ¡æ¶ˆæ¯é¿å…tokenè¿‡å¤š
                role = msg.get('role', '')
                content = msg.get('content', '')
                if role == 'user':
                    messages.append(HumanMessage(content=content))
                elif role == 'assistant':
                    messages.append(AIMessage(content=content))
        
        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        messages.append(HumanMessage(content=message))
        
        # æµå¼ç”Ÿæˆå“åº”
        for chunk in self.llm_streaming.stream(messages):
            if hasattr(chunk, 'content') and chunk.content:
                yield chunk.content

# =============================================================================
# 3. AgentService (The Conductor)
# è¿™æ˜¯ä¸€ä¸ªæ–°çš„æ ¸å¿ƒç±»ï¼Œè´Ÿè´£æ‰€æœ‰ç»„ä»¶çš„åˆå§‹åŒ–ã€ç®¡ç†å’ŒååŒå·¥ä½œã€‚
# =============================================================================

class AgentService:
    """æ™ºèƒ½ä½“æœåŠ¡çš„ä¸­å¤®åè°ƒå™¨ï¼ˆæ‡’åŠ è½½æ¨¡å¼ï¼‰"""
    def __init__(self, redis_config=None):
        print("åˆå§‹åŒ– AgentService...")
        self.config = ConfigManager()
        self.llm_factory = LLMFactory(self.config)
        self.mcp_manager = MCPManager(self.config)
        
        # æ‡’åŠ è½½ï¼šä¸åœ¨åˆå§‹åŒ–æ—¶ç«‹å³åŠ è½½MCPå·¥å…·
        self._mcp_tools = None
        self._tools_loaded = False
        
        # åˆå§‹åŒ–Redisè®°å¿†ç®¡ç†å™¨
        if redis_config is None:
            redis_config = {}
        self.redis_memory_manager = get_redis_memory_manager(**redis_config)
        
        self.agent_sessions: Dict[str, Dict[str, Any]] = {}
        print("AgentService åˆå§‹åŒ–å®Œæˆï¼ˆä½¿ç”¨æ‡’åŠ è½½æ¨¡å¼ + Redisè®°å¿†ï¼‰ã€‚")

    @property
    def mcp_tools(self) -> List[Any]:
        """æ‡’åŠ è½½MCPå·¥å…·"""
        if not self._tools_loaded:
            print("é¦–æ¬¡è¯·æ±‚MCPå·¥å…·ï¼Œå¼€å§‹åŠ è½½...")
            self._mcp_tools = self.mcp_manager.load_tools_sync()
            self._tools_loaded = True
            print(f"MCPå·¥å…·åŠ è½½å®Œæˆï¼Œå…± {len(self._mcp_tools)} ä¸ªå·¥å…·")
        return self._mcp_tools

    def _create_agent_session(self, user_email: str, conv_id: str) -> Dict[str, Any]:
        """ä¸ºæ–°ç”¨æˆ·åˆ›å»ºä¸€å¥—å®Œæ•´çš„æ™ºèƒ½ä½“å’Œè®°å¿†"""
        print(f"ä¸ºç”¨æˆ· {user_email} åˆ›å»ºæ–°çš„æ™ºèƒ½ä½“ Session...")
        llm_normal = self.llm_factory.create_llm(streaming=False)
        llm_streaming = self.llm_factory.create_llm(streaming=True)
        
        # åˆ›å»ºä¼šè¯ç‰¹å®šçš„è®°å¿†
        session_key = f"{user_email}_{conv_id}"
        memory = RedisSimpleMemory(session_key, self.redis_memory_manager)
        
        return {
            'collector': InformationCollectorAgent(llm_normal, self.mcp_tools),  # è¿™é‡Œæ‰ä¼šè§¦å‘å·¥å…·åŠ è½½
            'planner': PlannerAgent(llm_streaming, llm_normal),
            'pdf_agent': PdfAgent(llm_normal),
            'normal_agent': NormalAgent(llm_streaming),
            'memory': memory,
        }

    def get_or_create_agent_session(self, user_email: str, conv_id: str) -> Dict[str, Any]:
        """è·å–æˆ–åˆ›å»ºç”¨æˆ·çš„æ™ºèƒ½ä½“ä¼šè¯"""
        session_key = f"{user_email}_{conv_id}"
        if session_key not in self.agent_sessions:
            self.agent_sessions[session_key] = self._create_agent_session(user_email, conv_id)
        return self.agent_sessions[session_key]

    def get_response_stream(self, user_message: str, user_email: str, agent_type: str = "general", conv_id: Optional[str] = None):
        """å¤„ç†ç”¨æˆ·è¯·æ±‚å¹¶è¿”å›å“åº”æµï¼ˆæ”¯æŒRedisè®°å¿†ï¼‰"""
        if not conv_id:
            raise ValueError("Conversation ID (conv_id) ä¸èƒ½ä¸ºç©º")
            
        full_response = ""
        try:
            session = self.get_or_create_agent_session(user_email, conv_id)
            memory: RedisSimpleMemory = session['memory']
            
            # è·å–å¯¹è¯å†å²è®°å¿†
            conversation_history = memory.messages

            if agent_type == "general":
                agent = session['normal_agent']
                generator = agent.get_response_stream(user_message, conversation_history)
            
            elif agent_type == "travel":
                if is_travel_planning_request(user_message):
                    # Multi-agent workflow with memory
                    collector_agent = session['collector']
                    planner_agent = session['planner']
                    
                    print("æ—…è¡Œè§„åˆ’æµç¨‹: [1] ä¿¡æ¯æ”¶é›†ä¸­...")
                    collected_info = AsyncSyncWrapper.run_async_in_thread(lambda: collector_agent.collect_information_async(user_message))
                    print("æ—…è¡Œè§„åˆ’æµç¨‹: [2] å¼€å§‹æµå¼è§„åˆ’...")
                    generator = planner_agent.get_response_stream(user_message, collected_info, conversation_history)
                else:
                    # Simple travel question with memory
                    agent = session['normal_agent']
                    generator = agent.get_response_stream(user_message, conversation_history)

            elif agent_type == "pdf_generator":
                agent = session['pdf_agent']
                generator = agent.get_response_stream(user_message, conversation_history)
            
            else:
                raise ValueError(f"æœªçŸ¥çš„æ™ºèƒ½ä½“ç±»å‹: {agent_type}")

            # ä»ç”Ÿæˆå™¨æ¶ˆè´¹å†…å®¹å¹¶æ›´æ–°è®°å¿†
            for chunk in generator:
                full_response += chunk
                yield chunk
            
            # ä¿å­˜å¯¹è¯åˆ°Redisè®°å¿†ä¸­
            memory.add_message("user", user_message)
            memory.add_message("assistant", full_response)
            print(f"ğŸ’¾ å·²ä¿å­˜å¯¹è¯åˆ°Redisè®°å¿†ï¼Œå½“å‰è®°å¿†æ¡æ•°: {len(memory.messages)}")

        except Exception as e:
            error_msg = f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°äº†é—®é¢˜: {str(e)}"
            print(f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}\n{traceback.format_exc()}")
            yield error_msg
            
            # å³ä½¿å‡ºé”™ä¹Ÿä¿å­˜åˆ°è®°å¿†ä¸­
            try:
                memory.add_message("user", user_message)
                memory.add_message("assistant", error_msg)
            except:
                pass
    
    def clear_user_sessions(self, user_email: str) -> int:
        """æ¸…é™¤ç”¨æˆ·çš„æ‰€æœ‰ä¼šè¯è®°å¿†"""
        cleared_count = 0
        keys_to_delete = []
        
        # æ‰¾åˆ°æ‰€æœ‰éœ€è¦æ¸…é™¤çš„ä¼šè¯
        for session_key in list(self.agent_sessions.keys()):
            if session_key.startswith(user_email):
                keys_to_delete.append(session_key)
        
        # æ¸…é™¤å†…å­˜ä¸­çš„ä¼šè¯
        for key in keys_to_delete:
            if key in self.agent_sessions:
                # æ¸…é™¤Redisè®°å¿†
                memory = self.agent_sessions[key]['memory']
                memory.clear()
                # åˆ é™¤ä¼šè¯
                del self.agent_sessions[key]
                cleared_count += 1
        
        print(f"å·²æ¸…é™¤ç”¨æˆ· {user_email} çš„ {cleared_count} ä¸ªä¼šè¯è®°å¿†")
        return cleared_count
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """è·å–è®°å¿†ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.redis_memory_manager.get_memory_stats()
        stats["active_agent_sessions"] = len(self.agent_sessions)
        return stats

# =============================================================================
# 4. Main Service Instance and Compatibility Layer (The Public API)
# æ‡’åŠ è½½å…¨å±€å®ä¾‹ï¼šåªåœ¨éœ€è¦æ—¶æ‰åˆ›å»º
# =============================================================================

_agent_service = None

def get_agent_service(redis_config=None) -> AgentService:
    """è·å–å…¨å±€AgentServiceå®ä¾‹ï¼ˆæ‡’åŠ è½½ï¼‰"""
    global _agent_service
    if _agent_service is None:
        _agent_service = AgentService(redis_config=redis_config)
    return _agent_service

# --- æ—§å‡½æ•°æ¥å£ï¼Œç°åœ¨ä»£ç†åˆ° AgentService ---
def get_agent_response_stream(user_message, user_email, agent_type="general", conv_id=None):
    """
    [å…¼å®¹æ€§æ¥å£] è·å–æ™ºèƒ½ä½“å“åº”æµã€‚
    æ­¤å‡½æ•°ç°åœ¨æ˜¯ AgentService.get_response_stream çš„ä¸€ä¸ªç®€å•åŒ…è£…ã€‚
    """
    return get_agent_service().get_response_stream(user_message, user_email, agent_type, conv_id)

def load_mcp_tools_async():
    """[å…¼å®¹æ€§æ¥å£] å¼‚æ­¥åŠ è½½MCPå·¥å…·"""
    return get_agent_service().mcp_manager.load_tools_async()

def load_mcp_tools_sync():
    """[å…¼å®¹æ€§æ¥å£] åŒæ­¥åŠ è½½MCPå·¥å…·çš„åŒ…è£…å™¨"""
    return get_agent_service().mcp_manager.load_tools_sync()

def clear_user_agent_sessions(user_email: str) -> int:
    """[æ–°æ¥å£] æ¸…é™¤ç”¨æˆ·çš„æ‰€æœ‰æ™ºèƒ½ä½“ä¼šè¯å’ŒRedisè®°å¿†"""
    return get_agent_service().clear_user_sessions(user_email)

def get_agent_memory_stats() -> Dict[str, Any]:
    """[æ–°æ¥å£] è·å–æ™ºèƒ½ä½“è®°å¿†ç»Ÿè®¡ä¿¡æ¯"""
    return get_agent_service().get_memory_stats()

def is_travel_planning_request(message: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºæ—…è¡Œè§„åˆ’è¯·æ±‚"""
    travel_keywords = ['æ—…è¡Œ', 'æ—…æ¸¸', 'å‡ºè¡Œ', 'è¡Œç¨‹', 'è§„åˆ’', 'è®¡åˆ’', 'æœºç¥¨', 'é…’åº—', 'ä½å®¿', 'æ™¯ç‚¹', 'è·¯çº¿',
                       'travel', 'trip', 'vacation', 'itinerary', 'plan', 'flight', 'hotel', 'attraction', 'route']
    return any(keyword in message.lower() for keyword in travel_keywords)